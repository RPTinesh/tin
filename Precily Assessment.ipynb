{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824f7a34",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314c735",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "Given two paragraphs, quantify the degree of similarity between the two text-based on Semantic\n",
    "similarity. Semantic Textual Similarity (STS) assesses the degree to which two sentences are\n",
    "semantically equivalent to each other. The STS task is motivated by the observation that accurately\n",
    "modelling the meaning similarity of sentences is a foundational language understanding problem\n",
    "relevant to numerous applications including machine translation (MT), summarization, generation,\n",
    "question-answering (QA), short answer grading, semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b1833",
   "metadata": {},
   "source": [
    "# Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00e20ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf       # To work with Universal Sentence Encoder version 4\n",
    "import pandas as pd           # To work with dataframes\n",
    "import tensorflow_hub as hub  # contains USE4\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #Model is imported from this URL\n",
    "model = hub.load(module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e742d13",
   "metadata": {},
   "source": [
    "we have used Universal Sentence Encoder(USE). It encodes text into higher dimensional vectors that can be used for our semantic similarity task. The pre-trained Universal Sentence Encoder(USE) is publicly available in tensorflow hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a8a5d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>india seeks to boost construction india has cl...</td>\n",
       "      <td>music mogul fuller sells company pop idol supr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>podcasters  look to net money nasa is doing it...</td>\n",
       "      <td>ukip outspent labour on eu poll the uk indepen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>row over  police  power for csos the police fe...</td>\n",
       "      <td>ban on hunting comes into force fox hunting wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>election  could be terror target  terrorists m...</td>\n",
       "      <td>nhs waiting time target is cut hospital waitin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>japan economy slides to recession the japanese...</td>\n",
       "      <td>optimism remains over uk housing the uk proper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  savvy searchers fail to spot ads internet sear...   \n",
       "1          1  millions to miss out on the net by 2025  40% o...   \n",
       "2          2  young debut cut short by ginepri fifteen-year-...   \n",
       "3          3  diageo to buy us wine firm diageo  the world s...   \n",
       "4          4  be careful how you code a new european directi...   \n",
       "5          5  india seeks to boost construction india has cl...   \n",
       "6          6  podcasters  look to net money nasa is doing it...   \n",
       "7          7  row over  police  power for csos the police fe...   \n",
       "8          8  election  could be terror target  terrorists m...   \n",
       "9          9  japan economy slides to recession the japanese...   \n",
       "\n",
       "                                               text2  \n",
       "0  newcastle 2-1 bolton kieron dyer smashed home ...  \n",
       "1  nasdaq planning $100m share sale the owner of ...  \n",
       "2  ruddock backs yapp s credentials wales coach m...  \n",
       "3  mci shares climb on takeover bid shares in us ...  \n",
       "4  media gadgets get moving pocket-sized devices ...  \n",
       "5  music mogul fuller sells company pop idol supr...  \n",
       "6  ukip outspent labour on eu poll the uk indepen...  \n",
       "7  ban on hunting comes into force fox hunting wi...  \n",
       "8  nhs waiting time target is cut hospital waitin...  \n",
       "9  optimism remains over uk housing the uk proper...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/Admin/Downloads/Text_Similarity_Dataset (1).csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b1668c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'savvy searchers fail to spot ads internet search engine users are an odd mix of naive and sophisticated  suggests a report into search habits.  the report by the us pew research center reveals that 87% of searchers usually find what they were looking for when using a search engine. it also shows that few can spot the difference between paid-for results and organic ones. the report reveals that 84% of net users say they regularly use google  ask jeeves  msn and yahoo when online.  almost 50% of those questioned said they would trust search engines much less  if they knew information about who paid for results was being hidden. according to figures gathered by the pew researchers the average users spends about 43 minutes per month carrying out 34 separate searches and looks at 1.9 webpages for each hunt. a significant chunk of net users  36%  carry out a search at least weekly and 29% of those asked only look every few weeks. for 44% of those questioned  the information they are looking for is critical to what they are doing and is information they simply have to find.  search engine users also tend to be very loyal and once they have found a site they feel they can trust tend to stick with it. according to pew research 44% of searchers use just a single search engine  48% use two or three and a small number  7%  consult more than three sites. tony macklin  spokesman for ask jeeves  said the results reflected its own research which showed that people use different search engines because the way the sites gather information means they can provide different results for the same query. despite this liking for search sites half of those questioned said they could get the same information via other routes. a small number  17%  said they wouldn t really miss search engines if they did not exist. the remaining 33% said they could not live without search sites. more than two-thirds of those questioned  68%  said they thought that the results they were presented with were a fair and unbiased selection of the information on a topic that can be found on the net. alongside the growing sophistication of net users is a lack of awareness about paid-for results that many search engines provide alongside lists of websites found by indexing the web. of those asked  62% were unaware that someone has paid for some of the results they see when they carry out a search. only 18% of all searchers say they can tell which results are paid for and which are not. said the pew report:  this finding is ironic  since nearly half of all users say they would stop using search engines if they thought engines were not being clear about how they presented paid results.  commenting mr macklin said sponsored results must be clearly marked and though they might help with some queries user testing showed that people need to be able to spot the difference.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72307919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['text1'][0]) # we can see that all the data is in string type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf05a72",
   "metadata": {},
   "source": [
    "# Text to vectors\n",
    "USE version 4. It is trained on the whole wikipedia data. Our Sentence have a sequence of words. we give this sentence to our model (USE4), it gives us a \"dense numeric vector\". Here, we passed sentence pair and got a vector pair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd596723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       "array([[ 0.05397232, -0.04840362, -0.05309717, ...,  0.04776653,\n",
       "        -0.06002417, -0.02362861],\n",
       "       [-0.04064741, -0.05544911, -0.0575323 , ...,  0.05157086,\n",
       "        -0.05860625, -0.05815785]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = [data['text1'][0], data['text2'][0]]\n",
    "message_embeddings = embed(message)\n",
    "message_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db6991e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(message_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937ae1c",
   "metadata": {},
   "source": [
    "# Here we can see that the type of the vector retured is tensorflow.python.framework.ops.EagerTensor so, we cannot directly use it to compute the cosine similarity. We need to convert it into a numpy array first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "184a13de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(message_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3758e82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(tf.make_ndarray(tf.make_tensor_proto(message_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15919b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np = tf.make_ndarray(tf.make_tensor_proto(message_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c00483",
   "metadata": {},
   "source": [
    "# Proposed approach\n",
    " *The Universal Sentence Encoder version 4 makes getting sentence level embedding’s as easy as it has historically been to lookup the embedding’s for individual words. \n",
    " *Word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c384e8",
   "metadata": {},
   "source": [
    "# Finding Cosine similarity\n",
    "Created a loop for all sentence pair present in our data and found the vector representation of our sentences. For each vector pair, we found the cosine between the by using usual cosine formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954adcbe",
   "metadata": {},
   "source": [
    "# Formula for Cosine Similarity\n",
    "##cosin = dot(a,b)/norm(a)*norm(b)\n",
    "The values will be from -1 to 1.  But, we need values ranging from 0 to 1 hence we will add 1 to the cosine similarity value and then normalize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0634da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot          # to calculate the dot product of two vectors\n",
    "from numpy.linalg import norm  #for finding the norm of a vector\n",
    "\n",
    "ans = []                        # This list will contain the cosin similarity value for each vector pair present.\n",
    "for i in range(len(data)):\n",
    "  messages = [data['text1'][i], data['text2'][i]]               #storing each sentence pair in messages\n",
    "  message_embeddings = embed(messages)                          #converting the sentence pair to vector pair using the embed() function\n",
    "  a = tf.make_ndarray(tf.make_tensor_proto(message_embeddings)) #storing the vector in the form of numpy array\n",
    "  cos_sim = dot(a[0], a[1])/(norm(a[0])*norm(a[1]))             #Finding the cosine between the two vectors\n",
    "  ans.append(cos_sim)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2025c864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4023"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5fea51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = pd.DataFrame(ans, columns = ['Similarity_Score'])         #converting the ans list into Dataframe so that we can add it to our \"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a557806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similarity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.170659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.188169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.421391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.392460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Similarity_Score\n",
       "0          0.170659\n",
       "1          0.188169\n",
       "2          0.463088\n",
       "3          0.421391\n",
       "4          0.392460"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58ac50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = data.join(Ans)  #Joining the Similarity_Score Dataframe (Ans) to our main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "285c7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Similarity_Score'] = Data['Similarity_Score'] + 1  #adding 1 to each of the values of Similarity_Score to make the values from 0 to 2. (Initially it was from [-1,1])             #adding 1 to each of the values of Similarity_Score to make the values from 0 to 2. (Initially it was from [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12bcfc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Similarity_Score'] = Data['Similarity_Score']/Data['Similarity_Score'].abs().max() #Normalizing the Similarity_Score to get the value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "521e6241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>Similarity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "      <td>0.585329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "      <td>0.594085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "      <td>0.731544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "      <td>0.710695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "      <td>0.696230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  savvy searchers fail to spot ads internet sear...   \n",
       "1          1  millions to miss out on the net by 2025  40% o...   \n",
       "2          2  young debut cut short by ginepri fifteen-year-...   \n",
       "3          3  diageo to buy us wine firm diageo  the world s...   \n",
       "4          4  be careful how you code a new european directi...   \n",
       "\n",
       "                                               text2  Similarity_Score  \n",
       "0  newcastle 2-1 bolton kieron dyer smashed home ...          0.585329  \n",
       "1  nasdaq planning $100m share sale the owner of ...          0.594085  \n",
       "2  ruddock backs yapp s credentials wales coach m...          0.731544  \n",
       "3  mci shares climb on takeover bid shares in us ...          0.710695  \n",
       "4  media gadgets get moving pocket-sized devices ...          0.696230  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4e0aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Final_score = Data[['Unique_ID', 'Similarity_Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57f4f17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.585329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.594085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.731544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.710695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.696230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  Similarity_Score\n",
       "0          0          0.585329\n",
       "1          1          0.594085\n",
       "2          2          0.731544\n",
       "3          3          0.710695\n",
       "4          4          0.696230"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb97d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_score.to_csv('C:/Users/Admin/Downloads/Final_score.csv',index = False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
